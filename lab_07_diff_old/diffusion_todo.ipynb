{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c652933",
   "metadata": {
    "id": "1c652933"
   },
   "source": [
    "# Diffusion Models\n",
    "\n",
    "Diffusion models are generative models used primarily for image synthesis and other computer vision tasks. They are trained to **progressively add random noise** to data (the *forward diffusion* process) and then **learn to reverse** that process to generate high-quality samples from noise.\n",
    "\n",
    "## Before diffusion: GANs and VAEs\n",
    "\n",
    "**Generative Adversarial Networks (GANs).**\n",
    "GANs consist of two models trained together: a **generator**, which tries to produce realistic “fake” images, and a **discriminator**, which tries to distinguish generated images from real ones. Training is adversarial—the generator improves by learning to fool the discriminator.\n",
    "\n",
    "**Variational Autoencoders (VAEs).**\n",
    "A VAE encodes an input image into a **latent space** (a low-dimensional vector capturing salient features) and then decodes it back to reconstruct the image. Unlike standard autoencoders, VAEs are **probabilistic**: instead of encoding to a single fixed latent vector (z), they learn a **distribution** over latent variables (e.g., (p(z \\mid x))) and sample from it during training and generation.\n",
    "\n",
    "> This exercise is not about implementing GANs or VAEs, so we won’t dive into their details here.\n",
    "\n",
    "### Further reading\n",
    "\n",
    "* [What are generative adversarial networks (GANs)?](https://www.ibm.com/think/topics/generative-adversarial-networks)\n",
    "* [Generative Adversarial Network (GAN) — GeeksforGeeks](https://www.geeksforgeeks.org/deep-learning/generative-adversarial-network-gan/)\n",
    "* [What is a variational autoencoder?](https://www.ibm.com/think/topics/variational-autoencoder)\n",
    "* [Variational AutoEncoders — GeeksforGeeks](https://www.geeksforgeeks.org/machine-learning/variational-autoencoders/)\n",
    "\n",
    "## Rise of diffusion models\n",
    "\n",
    "In 2020, the paper **[Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)** introduced diffusion probabilistic models (DDPMs), which use the diffusion mechanism to generate images. Since then, diffusion has become one of the most popular families of generative models. Notable examples include **[SORA-1](https://openai.com/index/video-generation-models-as-world-simulators/)**, **[SORA-2](https://openai.com/index/sora-2/)**, **[DALL·E 2](https://openai.com/index/dall-e-2/)**, and the **Stable Diffusion** family.\n",
    "\n",
    "You can explore thousands of stable diffusion models on **[Hugging Face](https://huggingface.co/models?other=stable-diffusion)**.\n",
    "\n",
    "What is interesing, there are also new types of diffusion models: Diffusion LLMs to generate text faster then autoregresive models. Check [Inception Diffusion LLM](https://www.inceptionlabs.ai/).\n",
    "\n",
    "---\n",
    "\n",
    "This course mainly base on the following resources:\n",
    "\n",
    "* **Tutorial:** [Diffusion Model from Scratch in PyTorch (DDPM)](https://medium.com/data-science/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946)\n",
    "* **Original paper:** [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33054dd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4173,
     "status": "ok",
     "timestamp": 1761824946472,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "33054dd7",
    "outputId": "0131c76c-a681-4ad3-82fb-695b14fa85d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.19 environment at: /home/jrosa/AGH_FILES/GSN-2025W/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.10.19 environment at: /home/jrosa/AGH_FILES/GSN-2025W/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jrosa/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrosa\u001b[0m (\u001b[33mdeep-neural-network-course\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "!uv pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "!uv pip install lightning wandb matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as L\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "import wandb\n",
    "\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "wandb.login()  # Log in to your W&B account\n",
    "L.seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3b08e",
   "metadata": {
    "id": "b0e3b08e"
   },
   "source": [
    "# Diffusion model overview\n",
    "\n",
    "A diffusion model has two phases:\n",
    "1. **Forward (diffusion) process** – gradually corrupts clean data with noise until it becomes pure Gaussian noise.\n",
    "2. **Reverse (denoising) process** – learns to invert that corruption and turn noise back into data.\n",
    "\n",
    "---\n",
    "\n",
    "## Forward diffusion process\n",
    "\n",
    "The forward diffusion process takes a clean data sample $x_0$ (e.g. an image) and step by step adds Gaussian noise until it becomes nearly pure noise $x_T$.\n",
    "\n",
    "This process is modeled as a Markov chain. That means each noisy sample $x_t$ depends only on the previous one $x_{t-1}$, not on the full history:\n",
    "\n",
    "$$\n",
    "q(x_t \\mid x_{t-1}) = \\mathcal{N}\\left(\\sqrt{1 - \\beta_t}\\, x_{t-1}, \\ \\beta_t I \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\beta_t$ controls how much noise we add at timestep $t$.\n",
    "- Small $\\beta_t$ → a tiny amount of noise. Large $\\beta_t$ → more noise.\n",
    "- After many steps $t = 1 \\dots T$, the data loses structure and becomes close to pure Gaussian noise.\n",
    "\n",
    "Instead of simulating all steps one by one, we can directly sample the noised version at any timestep $t$ using a reparameterization. Define:\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "- $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$\n",
    "\n",
    "Then we can write:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\varepsilon\n",
    "$$\n",
    "\n",
    "with $\\varepsilon \\sim \\mathcal{N}(0, I)$.\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\bar{\\alpha}_t$ acts like a signal-to-noise ratio at step $t$.\n",
    "  - Early (small $t$): $\\bar{\\alpha}_t \\approx 1$ → mostly clean signal.\n",
    "  - Late (large $t$): $\\bar{\\alpha}_t \\approx 0$ → mostly noise.\n",
    "- This closed-form lets us sample $x_t$ at any timestep $t$ in one shot, without running every previous step. This is important for training efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## Reverse diffusion process\n",
    "\n",
    "The reverse diffusion process is the generative step. We start from pure noise $x_T \\sim \\mathcal{N}(0, I)$ and try to iteratively denoise it back into something that looks like real data.\n",
    "\n",
    "In an ideal world, we would sample\n",
    "\n",
    "$$\n",
    "q(x_{t-1} \\mid x_t)\n",
    "$$\n",
    "\n",
    "which answers: “given this noisy image $x_t$, what should the slightly cleaner image $x_{t-1}$ look like?”\n",
    "\n",
    "But the true reverse distribution $q(x_{t-1} \\mid x_t)$ is intractable: technically solvable, but requiring infinite time to compute.\n",
    "\n",
    "So instead we train a neural network $\\varepsilon_\\theta(x_t, t)$ to predict the noise that was added at step $t$. If we know the noise, we can remove it and step from $x_t$ to an estimate of $x_{t-1}$. This defines a learned reverse process\n",
    "\n",
    "$$\n",
    "p_\\theta(x_{t-1} \\mid x_t)\n",
    "$$\n",
    "\n",
    "which we use to walk backward from $x_T$ to something that looks like $x_0$.\n",
    "\n",
    "**Intuition:**\n",
    "- Forward: slowly add noise.\n",
    "- Reverse: learn how to remove that noise step by step.\n",
    "- After training, we can generate a *new* image by starting from random noise and denoising repeatedly.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss function for training\n",
    "\n",
    "Training tries to make the learned reverse process $p_\\theta$ match the true forward process $q$. The full derivation is written as a variational lower bound (VLB) and involves KL divergences between Gaussians at each step.\n",
    "\n",
    "It is commonly described in terms of three terms: $L_T$, $L_t$, and $L_0$.\n",
    "\n",
    "- **$L_T$**  \n",
    "  KL divergence between $q(x_T \\mid x_0)$ and $p_\\theta(x_T)$.  \n",
    "  Intuition: does the model's starting distribution at $t = T$ match pure Gaussian noise?  \n",
    "  This can usually be treated as a constant because $x_T$ is basically standard Gaussian.\n",
    "\n",
    "- **$L_t$**  \n",
    "  KL divergence between the *true* reverse step $q(x_{t-1} \\mid x_t, x_0)$ and the *learned* reverse step $p_\\theta(x_{t-1} \\mid x_t)$.  \n",
    "  This measures how well the model denoises at each timestep.\n",
    "\n",
    "- **$L_0$**  \n",
    "  Negative log-likelihood of reconstructing the original sample $x_0$ from $x_1$:  \n",
    "  $-\\log p_\\theta(x_0 \\mid x_1)$.  \n",
    "  This tells us how well the final denoised output matches real data.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical simplification\n",
    "\n",
    "All of that math can be simplified to a very convenient training loss.\n",
    "\n",
    "During training we:\n",
    "1. Take a real sample $x_0$.\n",
    "2. Sample a random timestep $t$.\n",
    "3. Generate $x_t$ using the closed form\n",
    "   $$\n",
    "   x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\varepsilon\n",
    "   $$\n",
    "   with known noise $\\varepsilon$.\n",
    "4. Ask the model to predict that noise: $\\varepsilon_\\theta(x_t, t)$.\n",
    "\n",
    "Then we optimize a simple mean squared error (MSE):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\text{simple}(\\theta)\n",
    "= \\mathbb{E}_{t, x_0, \\varepsilon}\n",
    "\\Big[\n",
    "\\| \\varepsilon - \\varepsilon_\\theta(x_t, t) \\|^2\n",
    "\\Big]\n",
    "$$\n",
    "\n",
    "So the model is literally trained to answer:  \n",
    "**\"What noise was added here?\"**\n",
    "\n",
    "---\n",
    "\n",
    "## Important to remember\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/vision-agh/DNN-Course-media/refs/heads/main/lab8_diffusion/figures/algorithms.png\" alt=\"Algorithms\" width=\"600\">\n",
    "\n",
    "- **Forward (diffusion) process:**  \n",
    "  We corrupt clean data into noise:\n",
    "  $$\n",
    "  x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\varepsilon\n",
    "  $$\n",
    "  where $\\bar{\\alpha}_t$ decreases over time → less signal, more noise → $x_T$ is almost pure Gaussian noise.\n",
    "\n",
    "- **Reverse (denoising) process:**  \n",
    "  A neural network predicts the noise $\\varepsilon_\\theta(x_t, t)$ so we can remove it and step from $x_t$ to $x_{t-1}$. Repeating this turns random noise into a realistic sample.\n",
    "\n",
    "\n",
    "- **Training loop (per batch image):**\n",
    "  1. Sample timestep $t$.\n",
    "  2. Create a noisy version $x_t$ from $x_0$.\n",
    "  3. Train the model to predict the noise that was added.\n",
    "\n",
    "- **Sampling / generation:**\n",
    "  1. Start from random Gaussian noise.\n",
    "  2. Apply the model step by step to denoise.\n",
    "  3. After all steps, you get a generated image.\n",
    "\n",
    "**Core idea:** diffusion learns how to undo noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26042bab",
   "metadata": {
    "id": "26042bab"
   },
   "source": [
    "### Implementation\n",
    "\n",
    "Let's start by implementing the **`DDPM_Scheduler`** class.\n",
    "This class will initialize the parameters $\\beta$ and $\\hat{\\alpha}$ used during training and sampling.\n",
    "\n",
    "In the original DDPM paper, *Ho et al.* used a **linear variance schedule** with 1,000 steps, where $\\beta_1 = 1 \\times 10^{-4}$ and $\\beta_T = 0.02$.\n",
    "Later studies found that alternative schedules — such as **cosine schedules** or even **learned schedules**  — can improve both performance and efficiency. But we will focus on basic one.\n",
    "\n",
    "**Steps to implement:**\n",
    "\n",
    "1. Initialize `self.beta` as a linearly spaced tensor (`torch.linspace`) over the number of timesteps, ranging from `1e-4` to `0.02`.\n",
    "2. Compute $\\alpha = 1 - \\beta$.\n",
    "3. Initialize `self.alpha` as the **cumulative product** of $\\alpha$ using `torch.cumprod`. Set requires_grad as `False`.\n",
    "\n",
    "We can define `forward` function, that takes parameter `t` and returns `self.beta` and `self.alpha` for timestamp `t`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5396aeb2",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761824946485,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "5396aeb2"
   },
   "outputs": [],
   "source": [
    "class DDPM_Scheduler(nn.Module):\n",
    "    def __init__(self, num_time_steps: int = 1000):\n",
    "        super().__init__()\n",
    "        self.beta = torch.linspace(1e-4, 0.02, num_time_steps)\n",
    "        alpha = 1 - self.beta\n",
    "        self.alpha = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "        # Ensure these tensors don't require gradients\n",
    "        self.beta.requires_grad_(False)\n",
    "        self.alpha.requires_grad_(False)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.beta[t], self.alpha[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65153f6e",
   "metadata": {
    "id": "65153f6e"
   },
   "source": [
    "## Model architecture (implementation notes)\n",
    "\n",
    "We now define the denoising network. Following the DDPM paper, the core model is a **U-Net** with:\n",
    "- **Residual convolutional blocks** (Wide-ResNet–style): two $3\\times3$ convs per block, **GroupNorm** instead of BatchNorm, **SiLU** nonlinearity, and **Dropout**.\n",
    "- **Self-attention blocks** at lower spatial resolutions (e.g., at $16\\times16$ for inputs of $32\\times32$ or $256\\times256$). Attention helps the model capture long-range dependencies that plain convolutions might miss.\n",
    "\n",
    "We’ll first implement two building blocks:\n",
    "1. `ResnetBlock2D` — a residual conv block that also injects **time embeddings**.\n",
    "2. `AttentionBlock` — a 2D self-attention block applied between down/up conv stages.\n",
    "\n",
    "---\n",
    "\n",
    "### `ResnetBlock2D` — residual block with time embedding\n",
    "\n",
    "**Constructor arguments.**\n",
    "- `C_in` *(int)*: input channel dimension.  \n",
    "- `C_out` *(int)*: output channel dimension.  \n",
    "- `C_emb` *(int)*: dimension of the **time embedding** that will be injected (described later).  \n",
    "- `num_groups` *(int)*: number of groups for `GroupNorm`.  \n",
    "- `dropout_p` *(float)*: dropout probability (e.g., `0.1` or `0.2`).\n",
    "\n",
    "**Layers inside.**\n",
    "- `GroupNorm(C_in)` → `Conv2d(C_in, C_out, 3×3, padding=1)`  \n",
    "- `Linear(C_emb → C_out)` to project the time embedding  \n",
    "- `GroupNorm(C_out)` → `Dropout(p=dropout_p)` → `Conv2d(C_out, C_out, 3×3, padding=1)`  \n",
    "- `SiLU(inplace = True)` nonlinearity after each normalization (pre-activation style)\n",
    "\n",
    "**Forward pass.**\n",
    "1. Normalize and activate the input.\n",
    "2. First conv.\n",
    "3. Inject time embedding:  \n",
    "   - Project: `emb = Linear(time_emb)`\n",
    "   - Broadcast and add: `h = h + emb[:, :, None, None]`\n",
    "4. Normalize → dropout → activate → second conv:  \n",
    "5. Residual add if `in` and `out` shapes match, else return output from second conv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1676f2",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761824946488,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "4b1676f2"
   },
   "outputs": [],
   "source": [
    "class ResnetBlock2D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 C_in: int,\n",
    "                 C_out: int,\n",
    "                 C_emb: int,\n",
    "                 num_groups: int,\n",
    "                 dropout_p: float):\n",
    "\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(num_groups, C_in)\n",
    "        self.conv1 = nn.Conv2d(C_in, C_out, kernel_size=3, padding=1)\n",
    "\n",
    "        self.time_emb_proj = nn.Linear(C_emb, C_out)\n",
    "\n",
    "        self.norm2 = nn.GroupNorm(num_groups, C_out)\n",
    "        self.conv2 = nn.Conv2d(C_out, C_out, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.nonlinearity = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, embeddings):\n",
    "        h = x\n",
    "        h = self.norm1(h)\n",
    "        h = self.nonlinearity(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        embeddings = self.time_emb_proj(embeddings)\n",
    "        h = h + embeddings[:, :, None, None]\n",
    "\n",
    "        h = self.norm2(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.nonlinearity(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        return h + x if h.shape == x.shape else h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e3a897",
   "metadata": {
    "id": "a7e3a897"
   },
   "source": [
    "### `AttentionBlock` — multi-head self-attention for 2D features\n",
    "\n",
    "**Constructor arguments.**\n",
    "- `C` *(int)*: channel dimension for input/output.  \n",
    "- `num_heads` *(int)*: number of attention heads; must divide `C`.  \n",
    "- `num_groups` *(int)*: number of groups for `GroupNorm`.  \n",
    "- `dropout_p` *(float)*: attention dropout probability.\n",
    "\n",
    "**Layers inside.**\n",
    "- `GroupNorm(num_groups, C)`  \n",
    "- `Linear(C → C)` for **query**, **key**, and **value** projections (separate layers)  \n",
    "- `Linear(C → C)` output projection (`proj_attn`)\n",
    "\n",
    "**Forward pass.**\n",
    "1. Normalize the input.  \n",
    "2. Flatten spatial dims for QKV: first use `.view` to merge `H*W` into `L`, then `.permute` to get `(B, L, C)` from `(B, C, H, W)`.  \n",
    "3. Project to Q, K, V using `x_flat` and the linear layers.  \n",
    "4. Reshape for multi-head attention: `.view(B, L, num_heads, head_dim)` then `.permute(0, 2, 1, 3)` to obtain `(B, num_heads, L, head_dim)`.  \n",
    "5. Compute attention with `F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p, is_causal=False)`.  \n",
    "6. Combine heads: `.permute(0, 2, 1, 3).contiguous()` back to `(B, L, num_heads, head_dim)`, then `.view(B, L, C)`.  \n",
    "7. Project the merged attention using the `proj_attn` linear layer.  \n",
    "8. Unflatten to the image shape: `.permute(0, 2, 1)` to `(B, C, L)`, then `.reshape(B, C, H, W)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ecd788e",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1761824946502,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "0ecd788e"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, C: int, num_heads: int, num_groups: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        assert C % num_heads == 0, \"C must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = C // num_heads\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups, C)\n",
    "\n",
    "        self.query = nn.Linear(C, C)\n",
    "        self.key = nn.Linear(C, C)\n",
    "        self.value = nn.Linear(C, C)\n",
    "\n",
    "        self.proj_attn = nn.Linear(C, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, H, W) - input feature map\n",
    "        Returns:\n",
    "            out: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        x_norm = self.group_norm(x)\n",
    "\n",
    "        # Flatten spatial dims: (B, C, H, W) -> (B, H*W, C)\n",
    "        x_flat = x_norm.view(B, C, H * W).permute(0, 2, 1)\n",
    "        L = x_flat.shape[1]\n",
    "\n",
    "        q = self.query(x_flat)\n",
    "        k = self.key(x_flat)\n",
    "        v = self.value(x_flat)\n",
    "\n",
    "        # Reshape for multi-head attention: (B, L, C) -> (B, num_heads, L, head_dim)\n",
    "        q = q.view(B, L, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.view(B, L, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, L, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        attn = F.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout_p, is_causal=False)\n",
    "\n",
    "        # Combine heads: (B, num_heads, L, head_dim) -> (B, L, C)\n",
    "        attn = attn.permute(0, 2, 1, 3).contiguous().view(B, L, C)\n",
    "        x_out = self.proj_attn(attn)\n",
    "\n",
    "        # Reshape back to image: (B, L, C) -> (B, C, H, W)\n",
    "        x_out = x_out.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a957781",
   "metadata": {
    "id": "5a957781"
   },
   "source": [
    "Now, using our ResNet and Attention block we will implement larger block for our U-Net, more precisly, DownBlock2D, UpBlock2D and MidBlock2D.\n",
    "\n",
    "### `DownBlock2D` — encoder stage with optional attention\n",
    "\n",
    "**Constructor arguments.**\n",
    "- `C_in` *(int)*: input channels.  \n",
    "- `C_out` *(int)*: output channels after this stage.  \n",
    "- `C_emb` *(int)*: time-embedding dimension passed to `ResnetBlock2D`.  \n",
    "- `num_groups` *(int)*: number of groups for `GroupNorm`.  \n",
    "- `dropout_p` *(float)*: dropout probability in residual blocks.  \n",
    "- `use_attn` *(bool)*: whether to insert an `AttentionBlock`.\n",
    "\n",
    "**Layers inside.**\n",
    "- `ResnetBlock2D(C_in → C_out)`  \n",
    "- `ResnetBlock2D(C_out → C_out)`  \n",
    "- `AttentionBlock(C_out, num_head=8)` *or* `Identity` (controlled by `use_attn`)  \n",
    "- Downsample: `Conv2d(C_out → C_out, kernel=3, stride=2, pad=1)`\n",
    "\n",
    "**Forward pass.**\n",
    "1. Apply two residual blocks with time conditioning.  \n",
    "2. Apply attention.  \n",
    "3. Save the current feature map as **`skip`** for the U-Net skip connection.  \n",
    "4. Downsample with a stride-2 conv.  \n",
    "5. Return `(x, skip)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd001cc",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761824946505,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "1dd001cc"
   },
   "outputs": [],
   "source": [
    "class DownBlock2D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 C_in,\n",
    "                 C_out,\n",
    "                 C_emb,\n",
    "                 num_groups,\n",
    "                 dropout_p,\n",
    "                 use_attn=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.use_attn = use_attn\n",
    "\n",
    "        self.res1 = ResnetBlock2D(C_in, C_out, C_emb, num_groups, dropout_p)\n",
    "        self.res2 = ResnetBlock2D(C_out, C_out, C_emb, num_groups, dropout_p)\n",
    "\n",
    "        if use_attn:\n",
    "            self.attn = AttentionBlock(C_out, num_heads=8, num_groups=num_groups, dropout_p=dropout_p)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "        self.down = nn.Conv2d(C_out, C_out, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        x = self.res1(x, emb)\n",
    "        x = self.res2(x, emb)\n",
    "        x = self.attn(x)\n",
    "        skip = x\n",
    "        x = self.down(x)\n",
    "        return x, skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859b1e8",
   "metadata": {
    "id": "e859b1e8"
   },
   "source": [
    "### `UpBlock2D` — decoder stage with skip fusion and optional attention\n",
    "\n",
    "**Constructor arguments.**\n",
    "- `C_in` *(int)*: channels of the incoming **upsampled** stream.  \n",
    "- `C_skip` *(int)*: channels from the encoder skip tensor (to be concatenated).  \n",
    "- `C_out` *(int)*: output channels after this stage.  \n",
    "- `C_emb` *(int)*: time-embedding dimension for `ResnetBlock2D`.  \n",
    "- `num_groups` *(int)*: number of groups for `GroupNorm`.  \n",
    "- `dropout_p` *(float)*: dropout probability in residual blocks.  \n",
    "- `use_attn` *(bool)*: whether to insert an `AttentionBlock`.\n",
    "\n",
    "**Layers inside.**\n",
    "- Upsample `up`: Sequential with `Upsample(scale_factor=2, mode=\"nearest\")` → `Conv2d(C_in → C_in, 3×3, pad=1)`  \n",
    "- `ResnetBlock2D(C_in + C_skip → C_out)`  \n",
    "- `ResnetBlock2D(C_out → C_out)`  \n",
    "- `AttentionBlock(C_out, num_heads=8)` *or* `Identity` (controlled by `use_attn`)\n",
    "\n",
    "**Forward pass.**\n",
    "1. Upsample the input stream (nearest) and smooth with a 3×3 conv.  \n",
    "2. If spatial sizes differ from the skip tensor `(x.shape[-2:] != skip.shape[-2:])`, **resize** with bilinear interpolation to match `(F.interpolate(x, size=skip.shape[-2:], mode=\"bilinear\", align_corners=False))`.  \n",
    "3. Concatenate channels: `cat([x, skip], dim=1)`.  \n",
    "4. Apply two residual blocks with time conditioning.  \n",
    "5. Apply attention.  \n",
    "6. Return the refined feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cfc74f9",
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1761824946548,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "2cfc74f9"
   },
   "outputs": [],
   "source": [
    "class UpBlock2D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 C_in,\n",
    "                 C_skip,\n",
    "                 C_out,\n",
    "                 C_emb,\n",
    "                 num_groups,\n",
    "                 dropout_p,\n",
    "                 use_attn=False):\n",
    "        super().__init__()\n",
    "        self.use_attn = use_attn\n",
    "\n",
    "        # Upsample: nearest neighbor + convolution\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(C_in, C_in, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # First resnet block takes concatenated input (C_in + C_skip) -> C_out\n",
    "        self.res1 = ResnetBlock2D(C_in + C_skip, C_out, C_emb, num_groups, dropout_p)\n",
    "\n",
    "        # Second resnet block maintains channels C_out -> C_out\n",
    "        self.res2 = ResnetBlock2D(C_out, C_out, C_emb, num_groups, dropout_p)\n",
    "\n",
    "        # Conditional attention\n",
    "        if use_attn:\n",
    "            self.attn = AttentionBlock(C_out, num_heads=8, num_groups=num_groups, dropout_p=dropout_p)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x, skip, emb):\n",
    "        x = self.up(x)\n",
    "        # safety resize if needed\n",
    "        if x.shape[-2:] != skip.shape[-2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res1(x, emb)\n",
    "        x = self.res2(x, emb)\n",
    "        x = self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8ad17",
   "metadata": {
    "id": "08d8ad17"
   },
   "source": [
    "### `MidBlock2D` — bottleneck with attention\n",
    "\n",
    "**Constructor arguments.**\n",
    "- `C` *(int)*: channel dimension throughout the mid block.  \n",
    "- `C_emb` *(int)*: time-embedding dimension for `ResnetBlock2D`.  \n",
    "- `num_groups` *(int)*: number of groups for `GroupNorm`.  \n",
    "- `dropout_p` *(float)*: dropout probability in residual blocks.\n",
    "\n",
    "**Layers inside.**\n",
    "- `ResnetBlock2D(C → C)`  \n",
    "- `AttentionBlock(C, num_heads=8)`  \n",
    "- `ResnetBlock2D(C → C)`\n",
    "\n",
    "**Forward pass.**\n",
    "1. Residual block with time conditioning.  \n",
    "2. Self-attention to inject global context.  \n",
    "3. Second residual block with time conditioning.  \n",
    "4. Return the bottleneck features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87339c95",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1761824946556,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "87339c95"
   },
   "outputs": [],
   "source": [
    "class MidBlock2D(nn.Module):\n",
    "    def __init__(self, C, C_emb, num_groups, dropout_p):\n",
    "        super().__init__()\n",
    "        self.res1 = ResnetBlock2D(C, C, C_emb, num_groups, dropout_p)\n",
    "        self.attn = AttentionBlock(C, num_heads=8, num_groups=num_groups, dropout_p=dropout_p)\n",
    "        self.res2 = ResnetBlock2D(C, C, C_emb, num_groups, dropout_p)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        x = self.res1(x, emb)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x, emb)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42dec6c",
   "metadata": {
    "id": "c42dec6c"
   },
   "source": [
    "### Time embeddings — providing timestep information\n",
    "\n",
    "The last part of our implementation is the **time embedding**, which provides each `ResnetBlock2D` with information about the current diffusion timestep.  \n",
    "This helps the model understand **how much noise** is present in the input and **how aggressively** it should denoise.  \n",
    "While not strictly required, adding time embeddings makes training **more stable** and generally improves the results.\n",
    "\n",
    "---\n",
    "\n",
    "### `SinusoidalEmbeddings`\n",
    "\n",
    "We define a class `SinusoidalEmbeddings`, similar to the **positional encodings** used in Transformers.  \n",
    "It maps a timestep index `t` to a continuous embedding vector that captures temporal information.\n",
    "\n",
    "**Constructor arguments.**\n",
    "- `num_time_steps` *(int)*: total number of diffusion steps (e.g., 1,000).  \n",
    "- `embed_dim` *(int)*: dimension of the generated embedding vector.\n",
    "\n",
    "**Forward pass.**\n",
    "- Takes the current timestep `t` (shape `[B]` or `[B, 1]`).\n",
    "- Returns the embedding tensor, moved to the same device as `t`.\n",
    "\n",
    "**Note.**  \n",
    "In this implementation, we do **not** need additional positional embeddings inside the self-attention layers.  \n",
    "Since each attention block is preceded by convolutional layers, those already provide strong **spatial positional informations** for the image patches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f535f0",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1761824946572,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "60f535f0"
   },
   "outputs": [],
   "source": [
    "class SinusoidalEmbeddings(nn.Module):\n",
    "    def __init__(self, time_steps:int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        position = torch.arange(time_steps).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n",
    "        embeddings = torch.zeros(time_steps, embed_dim, requires_grad=False)\n",
    "        embeddings[:, 0::2] = torch.sin(position * div)\n",
    "        embeddings[:, 1::2] = torch.cos(position * div)\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.embeddings.to(t.device)[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b79ae3",
   "metadata": {
    "id": "f0b79ae3"
   },
   "source": [
    "### `UNET` — the full diffusion model backbone\n",
    "\n",
    "Now it’s time to define the full **U-Net** model used in DDPMs.  \n",
    "This architecture combines all previously defined components — residual, attention, downsample, and upsample blocks — into one encoder–decoder network.\n",
    "\n",
    "---\n",
    "\n",
    "**Constructor arguments.**\n",
    "- `input_channels` *(int)*: number of input channels (1 for grayscale, 3 for RGB).  \n",
    "- `time_steps` *(int)*: total number of diffusion steps (for time embeddings).  \n",
    "- `base` *(int)*: base channel multiplier controlling network width.  \n",
    "- `C_emb` *(int)*: dimension of the time embedding.  \n",
    "- `num_groups` *(int)*: number of groups for `GroupNorm`.  \n",
    "- `dropout_prob` *(float)*: dropout probability for residual blocks.\n",
    "\n",
    "---\n",
    "\n",
    "**Architecture overview.**\n",
    "\n",
    "- **Time embeddings:**  \n",
    "  Each timestep $t$ is encoded into a sinusoidal embedding using `SinusoidalEmbeddings`, which conditions all `ResnetBlock2D` layers.\n",
    "\n",
    "- **Input & output heads:**  \n",
    "  - `conv_in`: initial $3×3$ convolution to project input to `base` channels.  \n",
    "  - `norm_out` → `SiLU` → `conv_out 3x3`: final normalization, activation, and projection back to `input_channels`.\n",
    "\n",
    "- **Encoder (Down path):**  \n",
    "  Series of `DownBlock2D`s that progressively reduce spatial size and increase channel depth:  \n",
    "  - `down1`: no attention  \n",
    "  - `down2`: includes attention  \n",
    "  - `down3`: no attention  \n",
    "\n",
    "- **Bottleneck (Middle block):**  \n",
    "  `MidBlock2D` with a self-attention layer at the lowest resolution.\n",
    "\n",
    "- **Decoder (Up path):**  \n",
    "  Series of `UpBlock2D`s that upsample feature maps and fuse skip connections from the encoder:  \n",
    "  - `up1`: no attention  \n",
    "  - `up2`: includes attention  \n",
    "  - `up3`: no attention  \n",
    "\n",
    "---\n",
    "\n",
    "**Forward pass.**\n",
    "1. Input `x` of shape `(B, input_channels, H, W)` is projected with `conv_in`.  \n",
    "2. Compute time embedding `emb = SinusoidalEmbeddings(t)`.  \n",
    "3. **Encoder:**  \n",
    "   - `h, skip1 = down1(h, emb)`\n",
    "   - `h, skip2 = down2(h, emb)`\n",
    "   - `h, skip3 = down3(h, emb)`\n",
    "4. **Bottleneck:**  \n",
    "   - `h = mid(h, emb)`\n",
    "5. **Decoder:**  \n",
    "   - `h = up1(h, skip3, emb)`\n",
    "   - `h = up2(h, skip2, emb)`\n",
    "   - `h = up3(h, skip1, emb)`\n",
    "6. **Output:**  \n",
    "   Normalize → activate → final convolution → output prediction `out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeee9838",
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1761824962884,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "aeee9838"
   },
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels: int = 1,\n",
    "                 time_steps: int = 1000,\n",
    "                 base: int = 128,\n",
    "                 C_emb: int = 512,\n",
    "                 num_groups: int = 32,\n",
    "                 dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Time/step embeddings\n",
    "        self.embeddings = SinusoidalEmbeddings(time_steps=time_steps, embed_dim=C_emb)\n",
    "\n",
    "        # Input & output heads\n",
    "        self.conv_in = nn.Conv2d(input_channels, base, kernel_size=3, padding=1)\n",
    "        self.norm_out = nn.GroupNorm(num_groups, base)\n",
    "        self.nonlinear = nn.SiLU(inplace=True)\n",
    "        self.conv_out = nn.Conv2d(base, input_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # ---------- Encoder (down path) ----------\n",
    "        self.down1 = DownBlock2D(\n",
    "            C_in=base, C_out=base, C_emb=C_emb,\n",
    "            num_groups=num_groups, dropout_p=dropout_prob, use_attn=False\n",
    "        )\n",
    "        self.down2 = DownBlock2D(\n",
    "            C_in=base, C_out=base * 2, C_emb=C_emb,\n",
    "            num_groups=num_groups, dropout_p=dropout_prob, use_attn=True\n",
    "        )\n",
    "        self.down3 = DownBlock2D(\n",
    "            C_in=base * 2, C_out=base * 4, C_emb=C_emb,\n",
    "            num_groups=num_groups, dropout_p=dropout_prob, use_attn=False\n",
    "        )\n",
    "\n",
    "        # ---------- Middle bottleneck ----------\n",
    "        self.mid = MidBlock2D(\n",
    "            C=base * 4, C_emb=C_emb,\n",
    "            num_groups=num_groups, dropout_p=dropout_prob\n",
    "        )\n",
    "\n",
    "        # ---------- Decoder (up path) ----------\n",
    "        self.up1 = UpBlock2D(\n",
    "            C_in=base * 4, C_skip=base * 4, C_out=base * 2, C_emb=C_emb,\n",
    "            num_groups=num_groups, dropout_p=dropout_prob, use_attn=False\n",
    "        )\n",
    "        self.up2 = UpBlock2D(\n",
    "            C_in=base * 2, C_skip=base * 2, C_out=base, C_emb=C_emb,\n",
    "            num_groups=num_groups, dropout_p=dropout_prob, use_attn=True\n",
    "        )\n",
    "        self.up3 = UpBlock2D(\n",
    "            C_in=base, C_skip=base, C_out=base, C_emb=C_emb,\n",
    "            num_groups=num_groups, dropout_p=dropout_prob, use_attn=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, input_channels, H, W)\n",
    "        t: (B,) integer timesteps compatible with SinusoidalEmbeddings\n",
    "        \"\"\"\n",
    "        B, _, H, W = x.shape\n",
    "        h = self.conv_in(x)\n",
    "        emb = self.embeddings(t).to(h.device)\n",
    "\n",
    "        # ---------- Down path (encoder) ----------\n",
    "        h, skip1 = self.down1(h, emb)   # (B, base, H/2, W/2)\n",
    "        h, skip2 = self.down2(h, emb)   # (B, 2*base, H/4, W/4)\n",
    "        h, skip3 = self.down3(h, emb)   # (B, 4*base, H/8, W/8)\n",
    "\n",
    "        # ---------- Bottleneck ----------\n",
    "        h = self.mid(h, emb)\n",
    "\n",
    "        # ---------- Up path (decoder) ----------\n",
    "        h = self.up1(h, skip3, emb)     # (B, 2*base, H/4, W/4)\n",
    "        h = self.up2(h, skip2, emb)     # (B, base, H/2, W/2)\n",
    "        h = self.up3(h, skip1, emb)     # (B, base, H, W)\n",
    "\n",
    "        # ---------- Output ----------\n",
    "        h = self.norm_out(h)\n",
    "        h = self.nonlinear(h)\n",
    "        out = self.conv_out(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f05596",
   "metadata": {
    "id": "21f05596"
   },
   "source": [
    "Quick check if we have same output shape as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59cb316",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2350,
     "status": "ok",
     "timestamp": 1761824969031,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "e59cb316",
    "outputId": "cf0799fc-ed55-4ac7-dcfb-318c4e2af237"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNET()\n",
    "x = torch.rand((16, 1, 28, 28))\n",
    "t = torch.randint(0,1000,(16,))\n",
    "model(x, t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33170383",
   "metadata": {
    "id": "33170383"
   },
   "source": [
    "### `DiffusionModel` — training and sampling with PyTorch Lightning\n",
    "\n",
    "The final step is wrapping our U-Net and scheduler into a complete **training module**.  \n",
    "The `DiffusionModel` class manages training, optimization, and sample generation using the PyTorch Lightning framework.\n",
    "\n",
    "**Main components.**\n",
    "- `UNET`: the denoising backbone predicting the added noise at each timestep.  \n",
    "- `DDPM_Scheduler`: precomputes $\\beta_t$ and $\\alpha_t$ schedules for noise addition.  \n",
    "- `MSELoss`: the DDPM training objective — model predicts the noise added to each image.  \n",
    "- Fixed noise buffers (`fixed_noise`, `fixed_noise_seq`) for deterministic sampling and logging.\n",
    "\n",
    "\n",
    "### **Training step**\n",
    "\n",
    "The model learns to predict the noise $\\varepsilon$ added during the forward diffusion process. This directly implements the simplified DDPM loss from Ho *et al.*, 2020.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Sample a batch of clean images `x` from the dataset.  \n",
    "2. Randomly pick diffusion timesteps `t ∈ [0, T)`.  \n",
    "3. Sample random Gaussian noise $`\\varepsilon ∼ N(0, I)`$.  \n",
    "4. Generate noisy images using:\n",
    "   $$\n",
    "   x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\varepsilon\n",
    "   $$\n",
    "5. Feed the noisy image and timestep to the model:\n",
    "   $$\n",
    "   \\hat{\\varepsilon}_\\theta = \\text{model}(x_t, t)\n",
    "   $$\n",
    "6. Compute the MSE loss:\n",
    "   $$\n",
    "   \\mathcal{L} = \\| \\varepsilon - \\hat{\\varepsilon}_\\theta \\|^2\n",
    "   $$\n",
    "7. Log training metrics (`train_loss`) for monitoring.\n",
    "\n",
    "\n",
    "### **Sampling (image generation)**\n",
    "\n",
    "The `generate_samples` method runs the **reverse diffusion process**, turning pure noise into images.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Start with Gaussian noise `z_T ∼ N(0, I)`.  \n",
    "2. Iterate backward over timesteps `t = T … 1`:  \n",
    "   - Predict the noise $\\hat{\\varepsilon}_\\theta = \\text{model}(z_t, t)$  \n",
    "   - Estimate the denoised image:\n",
    "     $$\n",
    "     z_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\Big(z_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\hat{\\varepsilon}_\\theta \\Big) + \\sqrt{\\beta_t} \\, \\varepsilon_t\n",
    "     $$\n",
    "     where $\\varepsilon_t$ is Gaussian noise (set to zero for deterministic sampling).\n",
    "3. Return final output `z_0` (the generated image).  \n",
    "4. Clamp pixel values to `[0, 1]` for valid image ranges.\n",
    "\n",
    "**Modes:**\n",
    "- `deterministic=True`: use fixed stored noise (reproducible samples).  \n",
    "- `deterministic=False`: randomize for diversity.\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/vision-agh/DNN-Course-media/refs/heads/main/lab8_diffusion/figures/algorithms.png\" alt=\"Algorithms\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eaa3abd",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1761825315234,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "9eaa3abd"
   },
   "outputs": [],
   "source": [
    "class DiffusionModel(L.LightningModule):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 1,\n",
    "                 img_shape: Tuple = (28, 28),\n",
    "                 num_time_steps: int = 1000,\n",
    "                 lr: float = 2e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        self.model = UNET(input_channels=in_channels, time_steps=num_time_steps)\n",
    "        self.scheduler = DDPM_Scheduler(num_time_steps=num_time_steps)\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "        self.lr = lr\n",
    "\n",
    "        self.register_buffer(\"fixed_noise\", torch.randn(16, in_channels, *img_shape))\n",
    "        self.register_buffer(\"fixed_noise_seq\", torch.randn(num_time_steps, 16, in_channels, *img_shape))\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        B = x.size(0)\n",
    "        t = torch.randint(0, self.num_time_steps, (B,), device=self.device)\n",
    "        e = torch.randn_like(x)\n",
    "        a = self.scheduler.alpha.to(self.device)[t].view(B, 1, 1, 1)\n",
    "        x_noisy = (torch.sqrt(a) * x) + (torch.sqrt(1 - a) * e)\n",
    "\n",
    "        output = self.model(x_noisy, t)\n",
    "        loss = self.criterion(output, e)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "        # Cosine annealing scheduler (smoothly decreases LR during training)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.trainer.max_epochs,\n",
    "            eta_min=self.lr * 0.1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_samples(self, n_samples=16, num_time_steps=1000, deterministic=True):\n",
    "        self.model.eval()\n",
    "\n",
    "        if deterministic:\n",
    "            z = self.fixed_noise.clone().to(self.device)\n",
    "        else:\n",
    "            z = torch.randn(n_samples, self.in_channels, *self.img_shape).to(self.device)\n",
    "\n",
    "        scheduler = self.scheduler\n",
    "\n",
    "        for t in reversed(range(1, num_time_steps)):\n",
    "            t_tensor = torch.tensor([t], device=z.device).repeat(n_samples)\n",
    "            beta_t = scheduler.beta.to(self.device)[t_tensor].view(n_samples, 1, 1, 1)\n",
    "            alpha_t = 1 - beta_t\n",
    "            hat_alpha_t = scheduler.alpha.to(self.device)[t_tensor].view(n_samples, 1, 1, 1)\n",
    "\n",
    "            z = (1 / sqrt(α_t)) * (z - ((1 - α_t) / sqrt(1 - ā_t)) * predicted_noise)\n",
    "            z = z + noise * sqrt(β_t)\n",
    "\n",
    "            if deterministic:\n",
    "                e = self.fixed_noise_seq[t].clone().to(self.device)\n",
    "            else:\n",
    "                e = torch.randn_like(z)\n",
    "\n",
    "            z = z + e * torch.sqrt(beta_t)\n",
    "        return z.clamp(0, 1)\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"Log generated samples to W&B every few epochs\"\"\"\n",
    "        imgs = self.generate_samples(n_samples=16)\n",
    "        grid = make_grid(imgs, nrow=4, normalize=True)\n",
    "        self.logger.experiment.log({\n",
    "            \"generated_samples\": [wandb.Image(grid, caption=f\"Epoch {self.current_epoch+1}\")]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd403ed",
   "metadata": {
    "id": "cdd403ed"
   },
   "source": [
    "We will train our model on MNIST dataset, we will use only Training split.\n",
    "\n",
    "We initialize WandbLogger from Pytorch Lightning, our DiffusionModel, checkpoint for saving weights based on training loss and we can start training.\n",
    "\n",
    "**Important!**\n",
    "\n",
    "You can train model for 10/25 epochs to reduce time and check if it works and then load pretrained weights from here [link](https://drive.google.com/drive/folders/1kGQhn7gRQ2YvKLYduWGArTfTBk20JR-J?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd2cda2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3469,
     "status": "ok",
     "timestamp": 1761825394781,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "bd2cda2f",
    "outputId": "c2dc2527-18e7-4203-d24d-1f1ebc9b647a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 10.5MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 224kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.02MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 20.2MB/s]\n",
      "/home/jrosa/AGH_FILES/GSN-2025W/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:508: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "You passed a `lightning.pytorch` object (ModelCheckpoint) to a `pytorch_lightning` Trainer. Please switch to a single import style.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m DiffusionModel(num_time_steps\u001b[38;5;241m=\u001b[39mnum_time_steps, lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     21\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m     22\u001b[0m         dirpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m         filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiffusion-\u001b[39m\u001b[38;5;132;01m{epoch:02d}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{train_loss:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[0;32m---> 29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m16-mixed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_every_n_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# trainer.fit(model, train_loader)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/AGH_FILES/GSN-2025W/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AGH_FILES/GSN-2025W/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:434\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir, model_registry)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate_grad_batches \u001b[38;5;241m=\u001b[39m accumulate_grad_batches\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# init callbacks\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Declare attributes to be set in _callback_connector on_trainer_init\u001b[39;00m\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trainer_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_checkpointing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_root_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_model_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# init data flags\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_val_every_n_epoch: Optional[\u001b[38;5;28mint\u001b[39m]\n",
      "File \u001b[0;32m~/AGH_FILES/GSN-2025W/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:82\u001b[0m, in \u001b[0;36m_CallbackConnector.on_trainer_init\u001b[0;34m(self, callbacks, enable_checkpointing, enable_progress_bar, default_root_dir, enable_model_summary, max_time)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_model_summary_callback(enable_model_summary)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mextend(_load_external_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_lightning.callbacks_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 82\u001b[0m \u001b[43m_validate_callbacks_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# push all model checkpoint callbacks to the end\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# it is important that these are the last callbacks to run\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reorder_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mcallbacks)\n",
      "File \u001b[0;32m~/AGH_FILES/GSN-2025W/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:248\u001b[0m, in \u001b[0;36m_validate_callbacks_list\u001b[0;34m(callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_callbacks_list\u001b[39m(callbacks: \u001b[38;5;28mlist\u001b[39m[Callback]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     stateful_callbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks \u001b[38;5;28;01mif\u001b[39;00m is_overridden(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m, instance\u001b[38;5;241m=\u001b[39mcb)]\n\u001b[1;32m    249\u001b[0m     seen_callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m stateful_callbacks:\n",
      "File \u001b[0;32m~/AGH_FILES/GSN-2025W/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:248\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_callbacks_list\u001b[39m(callbacks: \u001b[38;5;28mlist\u001b[39m[Callback]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     stateful_callbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_overridden\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    249\u001b[0m     seen_callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m stateful_callbacks:\n",
      "File \u001b[0;32m~/AGH_FILES/GSN-2025W/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/model_helpers.py:41\u001b[0m, in \u001b[0;36mis_overridden\u001b[0;34m(method_name, instance, parent)\u001b[0m\n\u001b[1;32m     39\u001b[0m         parent \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mCallback\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m         \u001b[43m_check_mixed_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a parent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_utilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_overridden \u001b[38;5;28;01mas\u001b[39;00m _is_overridden\n",
      "File \u001b[0;32m~/AGH_FILES/GSN-2025W/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/model_helpers.py:96\u001b[0m, in \u001b[0;36m_check_mixed_imports\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(instance)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) to a `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Trainer. Please switch to a single import style.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: You passed a `lightning.pytorch` object (ModelCheckpoint) to a `pytorch_lightning` Trainer. Please switch to a single import style."
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_workers = 16\n",
    "num_time_steps = 1000\n",
    "lr = 2e-5\n",
    "\n",
    "# Dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "# W&B logger\n",
    "wandb_logger = WandbLogger(project=\"lab8-diffusion\",\n",
    "                           entity=\"deep-neural-network-course\",\n",
    "                           group=\"diffusion-model\",\n",
    "                           name= \"Jan Rosa\",\n",
    "                           log_model=True)\n",
    "\n",
    "# Model + Trainer setup\n",
    "model = DiffusionModel(num_time_steps=num_time_steps, lr=lr)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"./checkpoints\",\n",
    "        filename=\"diffusion-{epoch:02d}-{train_loss:.4f}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"train_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=75,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=50\n",
    ")\n",
    "\n",
    "# trainer.fit(model, train_loader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b19017",
   "metadata": {
    "id": "50b19017"
   },
   "source": [
    "Generate examples from pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77329dbc",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1761824946591,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "77329dbc"
   },
   "outputs": [],
   "source": [
    "model = DiffusionModel.load_from_checkpoint('checkpoints/diffusion-epoch=44-train_loss=0.0138.ckpt', map_location=device)\n",
    "out = model.generate_samples(deterministic=False)\n",
    "grid = make_grid(out, nrow=4, normalize=True)\n",
    "grid = grid.cpu().permute(1, 2, 0).numpy()\n",
    "plt.imshow(grid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e187634",
   "metadata": {
    "id": "4e187634"
   },
   "source": [
    "And also we can visualise process of denoising sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2011e48",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1761824946592,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "a2011e48"
   },
   "outputs": [],
   "source": [
    "def display_reverse(images: List[torch.Tensor]):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(len(images), 1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        x = images[i].squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        ax.imshow(x, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def inference(checkpoint: str, num_time_steps: int = 1000):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DiffusionModel.load_from_checkpoint(checkpoint, map_location=device)\n",
    "    scheduler = DDPM_Scheduler(num_time_steps=num_time_steps)\n",
    "\n",
    "    times = [0, 15, 50, 100, 200, 300, 400, 550, 700, 999]\n",
    "    images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        x = torch.randn(1, 1, 28, 28).to(device)\n",
    "        for t in reversed(range(1, num_time_steps)):\n",
    "            t_tensor = torch.tensor([t], device=x.device).repeat(1)\n",
    "            beta_t = scheduler.beta.to(device)[t_tensor].view(1, 1, 1, 1)\n",
    "            alpha_t = scheduler.alpha.to(device)[t_tensor].view(1, 1, 1, 1)\n",
    "            temp = beta_t / (torch.sqrt(1 - alpha_t) * torch.sqrt(1 - beta_t))\n",
    "\n",
    "            x = (1 / torch.sqrt(1 - beta_t)) * x - (temp * model.model(x, t_tensor))\n",
    "            e = torch.randn_like(x)\n",
    "            x = x + e * torch.sqrt(beta_t)\n",
    "\n",
    "            if t in times:\n",
    "                images.append(x.clone().detach().cpu())\n",
    "\n",
    "    final_img = x.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
    "    plt.imshow(final_img, cmap='gray')\n",
    "    plt.title(\"Final generated image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    display_reverse(list(images))\n",
    "\n",
    "inference(\"checkpoints/diffusion-epoch=44-train_loss=0.0138.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde967e4",
   "metadata": {
    "id": "cde967e4"
   },
   "source": [
    "### Conditional Diffusion Model — adding control to generation\n",
    "\n",
    "If everything has been implemented correctly, we should now be able to generate **clear images of digits** 🎉  \n",
    "That completes our **basic Diffusion Model**!\n",
    "\n",
    "---\n",
    "\n",
    "### Why conditional diffusion?\n",
    "\n",
    "In our current setup, sampling starts from **pure noise** — so each generated image is **completely random**.  \n",
    "We have no control over what kind of image appears at the output.\n",
    "\n",
    "To fix this, we can introduce **conditioning information**, such as a class label or a text embedding, that guides the denoising process toward a specific type of output.  \n",
    "For example:\n",
    "- Generating a **digit “7”** in MNIST, or  \n",
    "- Creating a **cat** image instead of a **dog**, or  \n",
    "- Using a **text prompt** like *“a sunset over the mountains”* in **Stable Diffusion**.\n",
    "\n",
    "This approach is called a **Conditional Diffusion Model** — the model learns to condition its generation on an additional input (class, text, or latent embedding).\n",
    "\n",
    "---\n",
    "\n",
    "### Real-world example: Stable Diffusion\n",
    "\n",
    "Let’s look at how this concept scales to larger, real-world text-to-image models such as [**Stable Diffusion**](https://stablediffusionweb.com/).\n",
    "\n",
    "Even though implementations differ, most modern conditional diffusion systems share **three core components**:\n",
    "\n",
    "1. **Text Encoder** — transforms text into embeddings that condition image generation.  \n",
    "   - Example: [**CLIP**](https://github.com/openai/CLIP) from OpenAI, which aligns text and image embeddings using Transformer encoders.  \n",
    "   - The output embedding serves as a *conditioning vector* for the diffusion model.\n",
    "\n",
    "2. **U-Net Denoising Model** — performs iterative denoising in latent space.  \n",
    "   - This is essentially our **U-Net**, but often extended with **cross-attention layers** to incorporate conditioning information (e.g., from text).\n",
    "\n",
    "3. **Variational Autoencoder (VAE)** — encodes high-resolution images into a smaller **latent space**, reducing memory and computation.  \n",
    "   - The diffusion process operates in this compressed latent space.  \n",
    "   - The final decoded result is obtained by running the **VAE decoder**.\n",
    "\n",
    "These three parts together allow for **efficient**, **high-resolution**, and **text-controlled** generation.\n",
    "\n",
    "---\n",
    "\n",
    "### Our simplified conditional model\n",
    "\n",
    "Since training a full-scale Stable Diffusion model is resource-intensive, we can instead **adapt our current diffusion model** to accept conditioning signals.  \n",
    "We’ll focus on adding **Cross-Attention** layers — a mechanism that lets the model *attend* to an external embedding (e.g. class label or text feature).\n",
    "\n",
    "---\n",
    "\n",
    "### `CrossAttentionBlock` — fusing conditioning information\n",
    "\n",
    "This block works similarly to our standard `AttentionBlock`, but with a few key modifications:\n",
    "\n",
    "1. We now have **separate channel dimensions** for queries (`C_q`) and for keys/values (`C_kv`).  \n",
    "2. The `key` and `value` linear layers map from `C_kv → C_q`, while `query` maps from `C_q → C_q`.  \n",
    "3. We **do not** use normalization here.  \n",
    "4. We **flatten only the query input** (`x_q`), while the conditioning input (`x_kv`) is already in the correct shape.  \n",
    "5. Each of the query, key, and value tensors is projected through its corresponding `Linear` layer.  \n",
    "6. We reshape them for multi-head attention based on their respective sequence lengths `L_q` and `L_ctx`.  \n",
    "7. We apply `F.scaled_dot_product_attention`, just as before, followed by recombining the heads and a final linear projection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc5dbc",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1761824946603,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "98cc5dbc"
   },
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, C_q: int, C_kv: int, num_heads: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        assert C_q % num_heads == 0, \"C_q must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = C_q // num_heads\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Linear projections for query (from image) and key/value (from embeddings)\n",
    "        self.query = # TODO       # C_q -> C_q\n",
    "        self.key = # TODO         # C_kv -> C_q\n",
    "        self.value = # TODO       # C_kv -> C_q\n",
    "\n",
    "        self.proj_out = # TODO     # C_q -> C_q\n",
    "\n",
    "    def forward(self, x_q, x_kv):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_q: (B, C_q, H, W) - query feature map\n",
    "            x_kv: (B, L_ctx, C_kv) - contextual embeddings\n",
    "        Returns:\n",
    "            out: (B, C_q, H, W)\n",
    "        \"\"\"\n",
    "        B, Cq, H, W = x_q.shape\n",
    "        B, L_ctx, Ckv = x_kv.shape\n",
    "\n",
    "        # Flatten spatial dims of query\n",
    "        q = # TODO  # (B, Cq, H, W) -> (B, L_q, C_q)\n",
    "        L_q = q.shape[1]\n",
    "\n",
    "        q = # TODO          # (B, L_q, C_q)\n",
    "        k = # TODO          # (B, L_ctx, C_q)\n",
    "        v = # TODO       # (B, L_ctx, C_q)\n",
    "\n",
    "        # Reshape for multi-head attention (same as in Attention block with correct L)\n",
    "        q = # TODO  # (B, H, L_q, d)\n",
    "        k = # TODO # (B, H, L_ctx, d)\n",
    "        v = # TODO\n",
    "\n",
    "        attn = # TODO # scaled dot product\n",
    "\n",
    "        attn = attn.permute(0, 2, 1, 3).contiguous().view(B, L_q, Cq)\n",
    "\n",
    "        out = self.proj_out(attn)\n",
    "        out = out.permute(0, 2, 1).reshape(B, Cq, H, W)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0744e89",
   "metadata": {
    "id": "c0744e89"
   },
   "source": [
    "We introduce this inside MidBlock2D, right after AttentionBlock.\n",
    "\n",
    "Define `CrossMidBlock2D` with additional `CrossAttentionBlock` with parameter `C_cls`. In forward use it after `self.attn` and remember to use `emb.cls.unsqueeze(1)`. Output of this Cross Attention goes to second ResNetBlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ba963",
   "metadata": {
    "executionInfo": {
     "elapsed": 4474,
     "status": "aborted",
     "timestamp": 1761824946606,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "0c5ba963"
   },
   "outputs": [],
   "source": [
    "class CrossMidBlock2D(nn.Module):\n",
    "    def __init__(self, C,\n",
    "                        C_emb,\n",
    "                        C_cls,\n",
    "                        num_groups,\n",
    "                        dropout_p):\n",
    "\n",
    "        super().__init__()\n",
    "        self.res1 = ResnetBlock2D(C,   C,   C_emb, num_groups, dropout_p)\n",
    "        self.attn = AttentionBlock(C, num_heads=8, num_groups=num_groups, dropout_p=dropout_p)\n",
    "        self.cross_attn = CrossAttentionBlock(C, C_cls, num_heads=8, dropout_p=dropout_p)\n",
    "        self.res2 = ResnetBlock2D(C,   C,   C_emb, num_groups, dropout_p)\n",
    "\n",
    "    def forward(self, x, emb, emb_cls):\n",
    "        x = self.res1(x, emb)\n",
    "        x = self.attn(x)\n",
    "        x = self.cross_attn(x, emb_cls.unsqueeze(1))\n",
    "        x = self.res2(x, emb)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2acd10f",
   "metadata": {
    "id": "a2acd10f"
   },
   "source": [
    "We now define our `ConditionalUNET`, which inherits from the `UNET` class.  \n",
    "In the `__init__` method, we replace the `mid` layer with our new `CrossMidBlock2D` **(cls and time embeddings size are equal to `C_emb`)**, and additionally define an `nn.Embedding` layer with 10 classes (since we are using the MNIST dataset) that maps class labels to the same embedding dimension `C_emb`.\n",
    "\n",
    "In the `forward` method, we add:\n",
    "\n",
    "```python\n",
    "emb_cls = self.cls_embeddings(cls.to(x.device))\n",
    "```\n",
    "\n",
    "and modify the bottleneck call to:\n",
    "\n",
    "```python\n",
    "h = self.mid(h, emb, emb_cls)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca15e7",
   "metadata": {
    "executionInfo": {
     "elapsed": 4474,
     "status": "aborted",
     "timestamp": 1761824946607,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "09ca15e7"
   },
   "outputs": [],
   "source": [
    "class ConditionalUNET(UNET):\n",
    "    def __init__(self,\n",
    "                 input_channels: int = 1,\n",
    "                 time_steps: int = 1000,\n",
    "                 base: int = 128,\n",
    "                 C_emb = 512,\n",
    "                 num_groups: int = 32,\n",
    "                 dropout_prob: float = 0.1):\n",
    "        super().__init__(input_channels=input_channels,\n",
    "                 time_steps=time_steps,\n",
    "                 base=base,\n",
    "                 C_emb = C_emb,\n",
    "                 num_groups=num_groups,\n",
    "                 dropout_prob=dropout_prob)\n",
    "\n",
    "        # Cls/step embeddings\n",
    "        self.cls_embeddings = nn.Embedding(10, C_emb)\n",
    "        self.mid = CrossMidBlock2D(base * 4, C_emb, C_emb, num_groups, dropout_prob)\n",
    "\n",
    "    def forward(self, x, t, cls):\n",
    "        \"\"\"\n",
    "        x: (B, input_channels, H, W)\n",
    "        t: (B,) integer timesteps compatible with SinusoidalEmbeddings\n",
    "        \"\"\"\n",
    "        B, _, H, W = x.shape\n",
    "        h = self.conv_in(x)\n",
    "        emb = self.embeddings(t).to(h.device)\n",
    "        emb_cls = self.cls_embeddings(cls.to(x.device))\n",
    "\n",
    "        # ---------- Down path (encoder) ----------\n",
    "        h, skip1 = self.down1(h, emb)   # (B, base, H/2, W/2)\n",
    "        h, skip2 = self.down2(h, emb)   # (B, 2*base, H/4, W/4)\n",
    "        h, skip3 = self.down3(h, emb)   # (B, 4*base, H/8, W/8)\n",
    "\n",
    "        # ---------- Bottleneck ----------\n",
    "        h = self.mid(h, emb, emb_cls)\n",
    "\n",
    "        # ---------- Up path (decoder) ----------\n",
    "        h = self.up1(h, skip3, emb)     # (B, 2*base, H/4, W/4)\n",
    "        h = self.up2(h, skip2, emb)     # (B, base, H/2, W/2)\n",
    "        h = self.up3(h, skip1, emb)     # (B, base, H, W)\n",
    "\n",
    "        # ---------- Output ----------\n",
    "        h = self.norm_out(h)\n",
    "        h = self.nonlinear(h)\n",
    "        out = self.conv_out(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c25b56",
   "metadata": {
    "id": "19c25b56"
   },
   "source": [
    "And also we define the `ConditionalDiffusionModel` which simply use `y` values from MNIST dataset or arrange in sampling method inside model as additional parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42a7ff",
   "metadata": {
    "executionInfo": {
     "elapsed": 4474,
     "status": "aborted",
     "timestamp": 1761824946608,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "ee42a7ff"
   },
   "outputs": [],
   "source": [
    "class ConditionalDiffusionModel(L.LightningModule):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 1,\n",
    "                 img_shape: Tuple = (28, 28),\n",
    "                 num_time_steps: int = 1000,\n",
    "                 lr: float = 2e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        self.model = ConditionalUNET(input_channels=in_channels, time_steps=num_time_steps)\n",
    "        self.scheduler = DDPM_Scheduler(num_time_steps=num_time_steps)\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "        self.lr = lr\n",
    "\n",
    "        self.register_buffer(\"fixed_noise\", torch.randn(16, in_channels, *img_shape))\n",
    "        self.register_buffer(\"fixed_noise_seq\", torch.randn(num_time_steps, 16, in_channels, *img_shape))\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        B = x.size(0)\n",
    "        t = torch.randint(0, self.num_time_steps, (B,), device=self.device)\n",
    "        e = torch.randn_like(x)\n",
    "        a = self.scheduler.alpha.to(self.device)[t].view(B, 1, 1, 1)\n",
    "        x_noisy = (torch.sqrt(a) * x) + (torch.sqrt(1 - a) * e)\n",
    "\n",
    "        output = self.model(x_noisy, t, y)\n",
    "        loss = self.criterion(output, e)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "        # Cosine annealing scheduler (smoothly decreases LR during training)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.trainer.max_epochs,\n",
    "            eta_min=self.lr * 0.1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_samples(self, n_samples=16, num_time_steps=1000, deterministic=True, y=None):\n",
    "        self.model.eval()\n",
    "\n",
    "        if deterministic:\n",
    "            z = self.fixed_noise.clone().to(self.device)\n",
    "        else:\n",
    "            z = torch.randn(n_samples, self.in_channels, *self.img_shape).to(self.device)\n",
    "\n",
    "        scheduler = self.scheduler\n",
    "\n",
    "        if y is None:\n",
    "            y = torch.arange(n_samples, device=self.device) % 10\n",
    "\n",
    "        for t in reversed(range(1, num_time_steps)):\n",
    "            t_tensor = torch.tensor([t], device=z.device).repeat(n_samples)\n",
    "            beta_t = scheduler.beta.to(self.device)[t_tensor].view(n_samples, 1, 1, 1)\n",
    "            alpha_t = 1 - beta_t\n",
    "            hat_alpha_t = scheduler.alpha.to(self.device)[t_tensor].view(n_samples, 1, 1, 1)\n",
    "\n",
    "            z = # TODO # same as in non-conditional\n",
    "\n",
    "            if deterministic:\n",
    "                e = self.fixed_noise_seq[t].clone().to(self.device)\n",
    "            else:\n",
    "                e = torch.randn_like(z)\n",
    "\n",
    "            z = z + e * torch.sqrt(beta_t)\n",
    "        return z.clamp(0, 1)\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"Log generated samples to W&B every few epochs\"\"\"\n",
    "        imgs = self.generate_samples(n_samples=16)\n",
    "        grid = make_grid(imgs, nrow=4, normalize=True)\n",
    "        self.logger.experiment.log({\n",
    "            \"generated_samples\": [wandb.Image(grid, caption=f\"Epoch {self.current_epoch+1}\")]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd704fb",
   "metadata": {
    "id": "efd704fb"
   },
   "source": [
    "We now train our conditional model.  \n",
    "Although we could load the previously trained (unconditional) model weights, training this version from scratch is fast enough — especially on the MNIST dataset — so we’ll simply retrain it entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4e3ed",
   "metadata": {
    "executionInfo": {
     "elapsed": 4476,
     "status": "aborted",
     "timestamp": 1761824946610,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "01d4e3ed"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 16\n",
    "num_time_steps = 1000\n",
    "lr = 2e-5\n",
    "\n",
    "# Dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "# W&B logger\n",
    "wandb_logger = WandbLogger(project=\"lab8-diffusion\",\n",
    "                           entity=\"deep-neural-network-course\",\n",
    "                           group=\"conditional-diffusion-model\",\n",
    "                           name=# TODO (\"Name - conditional\")\n",
    "                           log_model=True)\n",
    "\n",
    "# Model + Trainer setup\n",
    "model = ConditionalDiffusionModel(num_time_steps=num_time_steps, lr=lr)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"./checkpoints\",\n",
    "        filename=\"conditional-diffusion-{epoch:02d}-{train_loss:.4f}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"train_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=75,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=50\n",
    ")\n",
    "\n",
    "# trainer.fit(model, train_loader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ae86d",
   "metadata": {
    "id": "060ae86d"
   },
   "source": [
    "Load conditional model weights and test result on defined `y` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58472b17",
   "metadata": {
    "executionInfo": {
     "elapsed": 4478,
     "status": "aborted",
     "timestamp": 1761824946612,
     "user": {
      "displayName": "JAN ROSA",
      "userId": "09972594920799623787"
     },
     "user_tz": -60
    },
    "id": "58472b17"
   },
   "outputs": [],
   "source": [
    "model = ConditionalDiffusionModel.load_from_checkpoint('checkpoints/conditional-diffusion-epoch=49-train_loss=0.0132.ckpt', map_location=device)\n",
    "y = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 8, 7, 6, 5, 4, 3, 2, 1], device=device)\n",
    "out = model.generate_samples(deterministic=False, y=y)\n",
    "grid = make_grid(out, nrow=4, normalize=True)\n",
    "grid = grid.cpu().permute(1, 2, 0).numpy()\n",
    "plt.imshow(grid)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GSN-2025W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
